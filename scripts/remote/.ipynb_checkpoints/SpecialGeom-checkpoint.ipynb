{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import itertools\n",
    "import PyVal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import ipyparallel as ipp\n",
    "import seaborn as sns\n",
    "import gzip\n",
    "from tqdm import tqdm_notebook\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/.local/lib/python3.6/site-packages/ipyparallel/client/client.py:459: RuntimeWarning: \n",
      "            Controller appears to be listening on localhost, but not on this machine.\n",
      "            If this is true, you should specify Client(...,sshserver='you@julian-ThinkPad-W530')\n",
      "            or instruct your controller to listen on an external IP.\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n",
      "sending file ../../external/lib/libtrng4.so.19 ./libtrng4.so.19\n",
      "None\n",
      "sending file ../../external/lib/libtrng4.so ./libtrng4.so\n",
      "None\n",
      "importing PyVal on engine(s)\n",
      "importing numpy on engine(s)\n"
     ]
    }
   ],
   "source": [
    "idx = pd.IndexSlice\n",
    "rc = ipp.Client()#('/home/julian/.ipython/profile_ssh/security/ipcontroller-client.json')\n",
    "dview = rc.load_balanced_view()\n",
    "print(rc.ids)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "def send_file(dview, local_path, remote_path):\n",
    "    \"\"\"Send a single file\"\"\"\n",
    "    print(\"sending file\", local_path, remote_path)\n",
    "    import io, os\n",
    "    \n",
    "    def _recv(remote_path, bdata, mtime):\n",
    "        import io, os\n",
    "        if os.path.exists(remote_path):\n",
    "            os.unlink(remote_path)\n",
    "        with io.open(remote_path, 'wb') as f:\n",
    "            f.write(data)\n",
    "        os.utime(remote_path, (mtime, mtime))\n",
    "    \n",
    "    st = os.stat(local_path)\n",
    "    with io.open(local_path, 'rb') as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    yield dview.apply_async(_recv, remote_path, data, st.st_mtime)\n",
    "    \n",
    "#TODO: use chrpath PyVal.so -r . to make libtrng visible\n",
    "#for el in send_file(dview, '../build/PyVal.cpython-36m-x86_64-linux-gnu.so','./PyVal.so'):\n",
    "    \n",
    "    print(el.get())\n",
    "for el in send_file(dview, '../../external/lib/libtrng4.so.19', './libtrng4.so.19'):\n",
    "    pass\n",
    "for el in send_file(dview, '../../external/lib/libtrng4.so', './libtrng4.so'):\n",
    "    pass\n",
    "\n",
    "with rc[:].sync_imports():\n",
    "    import PyVal\n",
    "    import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ipp.require('PyVal', 'numpy')\n",
    "indices = ['S0','T', 'r', 'N', 'col sum', 'default scale', 'conn', 'sigma']\n",
    "col_names = ['M', 'Assets', 'RS', 'Delta', 'Vega', 'Theta', 'Rho', 'Pi', 'Solvent']\n",
    "tmp_cols = ['N_MC', 'N_nets', 'Number Of Samples', 'p']\n",
    "\n",
    "def reduce_star_to_scalars(df, N, conn_scale):\n",
    "    #cols = df.columns.values\n",
    "    #cols = np.delete(cols, np.where((cols == 'RS') | (cols == 'Delta') | (cols == 'Delta var') | (cols == 'Vega') |\\\n",
    "    #                            (cols == 'Vega') | (cols == 'Vega var') | (cols == 'Rho') | (cols == 'Rho var') |\\\n",
    "    #                            (cols == 'Theta') | (cols == 'Theta var') | (cols == 'Pi') | (cols == 'Pi var') |\\\n",
    "    #                            (cols == 'M') | (cols == 'M var') | (cols == 'N_MC') | (cols == 'N_nets') |\\\n",
    "    #                            (cols == 'col sum') ))\n",
    "    cols =  np.array(['N', 'r', 'T', 'conn', 'default scale', 'col sum', 'S0', 'sigma',\\\n",
    "                      'Solvent', 'Solvent var', 'Assets', 'Assets var', 'R', 'S', \\\n",
    "                      'equity Delta', 'equity Delta var', 'debt Delta', 'debt Delta var',\\\n",
    "                      'equity Vega', 'equity Vega var', 'debt Vega', 'debt Vega var',\\\n",
    "                      'equity Rho', 'equity Rho var', 'debt Rho', 'debt Rho var',\\\n",
    "                      'equity Theta', 'equity Theta var', 'debt Theta', 'debt Theta var',\\\n",
    "                      'Pi', 'Pi var'])\n",
    "    df2 = pd.DataFrame(columns = cols, dtype=np.float64)\n",
    "    df2_debug = pd.DataFrame(columns = np.array(['M'])) #, 'io degree'\n",
    "    df2['Number Of Samples'] = df['Number Of Samples'].transform(lambda x: x[1])\n",
    "    df2['S'] = df['RS'].transform(lambda x: np.average(x[1:N]))\n",
    "    df2['R'] = df['RS'].transform(lambda x: np.average(x[N+1:]))\n",
    "    df2['S 0'] = df['RS'].transform(lambda x: np.average(x[0]))\n",
    "    df2['R 0'] = df['RS'].transform(lambda x: np.average(x[N]))\n",
    "    #df2['equity Delta'] = df['Delta'].transform(lambda x: np.sum(x[:N,1:])/(N-1))\n",
    "    #df2['equity Delta 0'] = df['Delta'].transform(lambda x: np.sum(x[:N,0]))\n",
    "    #df2['equity Delta var'] = df['Delta var'].transform(lambda x: np.sum(x[:N,:])/(N-1))\n",
    "    #df2['equity Delta 0 var'] = df['Delta var'].transform(lambda x: np.sum(x[:N,0]))\n",
    "    #df2['debt Delta'] = df['Delta'].transform(lambda x: np.sum(x[N:,1:])/(N-1))\n",
    "    #df2['debt Delta 0'] = df['Delta'].transform(lambda x: np.sum(x[N:,0]))\n",
    "    #df2['debt Delta var'] = df['Delta var'].transform(lambda x: np.sum(x[N:,1:])/(N-1))\n",
    "    #df2['debt Delta 0 var'] = df['Delta var'].transform(lambda x: np.sum(x[N:,0]))\n",
    "    for el in ['Delta', 'Vega', 'Rho', 'Theta']:\n",
    "        df2['equity ' + el] = df[el].transform(lambda x: np.sum(x[1:,1:N])/(N-1))\n",
    "        df2['equity ' + el + ' 0'] = df[el].transform(lambda x: np.sum(x[:,:1]))\n",
    "        df2['equity ' +el+ ' var'] = df[el+' var'].transform(lambda x: np.sum(x[:,1:N])/(N-1))\n",
    "        df2['equity ' +el+ ' var 0'] = df[el+' var'].transform(lambda x: np.sum(x[:,:1]))\n",
    "        df2['debt ' + el] = df[el].transform(lambda x: np.sum(x[:,N+1:])/(N-1))\n",
    "        df2['debt ' + el+' 0'] = df[el].transform(lambda x: np.sum(x[:,N]))\n",
    "        df2['debt ' +el+' var'] = df[el+' var'].transform(lambda x: np.sum(x[:,N+1:])/(N-1))\n",
    "        df2['debt ' +el+' var 0'] = df[el+' var'].transform(lambda x: np.sum(x[:,N]))\n",
    "    df2_debug['M'] = df['M']\n",
    "    #df2_debug['io degree'] = df['IO Degree Distribution']\n",
    "    df2['Assets'] = df['Assets'].transform(lambda x: np.average(x[1:N]))\n",
    "    df2['Assets 0'] = df['Assets'].transform(lambda x: x[0])\n",
    "    df2['Assets var'] = df['Assets var'].transform(lambda x: np.average(x[1:N]))\n",
    "    df2['Assets 0 var'] = df['Assets var'].transform(lambda x: x[0])\n",
    "    df2['Solvent'] = df['Solvent'].transform(lambda x: np.average(x[1:N]))\n",
    "    df2['Solvent 0'] = df['Solvent'].transform(lambda x: x[0])\n",
    "    df2['Solvent var'] = df['Solvent var'].transform(lambda x: np.average(x[1:N]))\n",
    "    df2['Solvent 0 var'] = df['Solvent var'].transform(lambda x: x[0])\n",
    "    df2['Pi'] = df['Pi'].transform(lambda x: np.average(x[1:N]))\n",
    "    df2['Pi 0'] = df['Pi'].transform(lambda x: x[0])\n",
    "    df2['Pi var'] = df['Pi var'].transform(lambda x: np.average(x[1:N]))\n",
    "    df2['Pi 0 var'] = df['Pi var'].transform(lambda x: x[0])\n",
    "    df2['N'] = df['N']\n",
    "    df2['r'] = df['r']\n",
    "    df2['T'] = df['T']\n",
    "    df2['conn'] = df['conn'].transform(lambda x: x/conn_scale)\n",
    "    df2['default scale'] = df['default scale']\n",
    "    df2['col sum'] = df['col sum']\n",
    "    df2['S0'] = df['S0']\n",
    "    df2['sigma'] = df['sigma']\n",
    "    df2['Network Type'] = df['Network Type']\n",
    "    #df2['p'] = df['p']\n",
    "    \n",
    "    return (df2, df2_debug)\n",
    "\n",
    "\n",
    "def reduce_to_scalars(df, N, conn_scale):\n",
    "    #cols = df.columns.values\n",
    "    #cols = np.delete(cols, np.where((cols == 'RS') | (cols == 'Delta') | (cols == 'Delta var') | (cols == 'Vega') |\\\n",
    "    #                            (cols == 'Vega') | (cols == 'Vega var') | (cols == 'Rho') | (cols == 'Rho var') |\\\n",
    "    #                            (cols == 'Theta') | (cols == 'Theta var') | (cols == 'Pi') | (cols == 'Pi var') |\\\n",
    "    #                            (cols == 'M') | (cols == 'M var') | (cols == 'N_MC') | (cols == 'N_nets') |\\\n",
    "    #                            (cols == 'col sum') ))\n",
    "    cols =  np.array(['Network Type', 'N', 'r', 'T', 'conn', 'default scale', 'col sum', 'S0', 'sigma',\\\n",
    "                      'Solvent', 'Solvent var', 'Assets', 'Assets var', 'R', 'S', \\\n",
    "                      'equity Delta', 'equity Delta var', 'debt Delta', 'debt Delta var',\\\n",
    "                      'equity Vega', 'equity Vega var', 'debt Vega', 'debt Vega var',\\\n",
    "                      'equity Rho', 'equity Rho var', 'debt Rho', 'debt Rho var',\\\n",
    "                      'equity Theta', 'equity Theta var', 'debt Theta', 'debt Theta var',\\\n",
    "                      'Pi', 'Pi var'])\n",
    "    df2 = pd.DataFrame(columns = cols, dtype=np.float64)\n",
    "    df2_debug = pd.DataFrame(columns = np.array(['M'])) #, 'io degree'\n",
    "    df2['Number Of Samples'] = df['Number Of Samples'].transform(lambda x: x[1])\n",
    "    df2['S'] = df['RS'].transform(lambda x: np.average(x[:N]))\n",
    "    df2['R'] = df['RS'].transform(lambda x: np.average(x[N:]))\n",
    "    df2['equity Delta'] = df['Delta'].transform(lambda x: np.sum(x[:N,:])/N)\n",
    "    df2['equity Delta var'] = df['Delta var'].transform(lambda x: np.sum(x[:N,:])/N)\n",
    "    df2['debt Delta'] = df['Delta'].transform(lambda x: np.sum(x[N:,:])/N)\n",
    "    df2['debt Delta var'] = df['Delta var'].transform(lambda x: np.sum(x[N:,:])/N)\n",
    "    for el in ['Vega', 'Rho', 'Theta']:\n",
    "        df2['equity ' + el] = df[el].transform(lambda x: np.sum(x[:,:N])/N)\n",
    "        df2['equity ' +el+ ' var'] = df[el+' var'].transform(lambda x: np.sum(x[:,:N])/N)\n",
    "        df2['debt ' + el] = df[el].transform(lambda x: np.sum(x[:,N:])/N)\n",
    "        df2['debt ' +el+' var'] = df[el+' var'].transform(lambda x: np.sum(x[:,N:])/N)\n",
    "        #if np.all(df2['debt ' + el] == 0.0) or np.all(df2['equity ' + el] == 0.):\n",
    "        #    print(\"WARNING::: check for correct dimensions ind reduce to scalars\")\n",
    "    df2_debug['M'] = df['M']\n",
    "    #df2_debug['io degree'] = df['IO Degree Distribution']\n",
    "    df2['Assets'] = df['Assets'].transform(lambda x: np.average(x))\n",
    "    df2['Assets var'] = df['Assets var'].transform(lambda x: np.average(x))\n",
    "    df2['Solvent'] = df['Solvent'].transform(lambda x: np.average(x))\n",
    "    df2['Solvent var'] = df['Solvent var'].transform(lambda x: np.average(x))\n",
    "    df2['Pi'] = df['Pi'].transform(lambda x: np.average(x))\n",
    "    df2['Pi var'] = df['Pi var'].transform(lambda x: np.average(x))\n",
    "    df2['N'] = df['N']\n",
    "    df2['r'] = df['r']\n",
    "    df2['T'] = df['T']\n",
    "    df2['conn'] = df['conn'].transform(lambda x: x/conn_scale)\n",
    "    df2['default scale'] = df['default scale']\n",
    "    df2['col sum'] = df['col sum']\n",
    "    df2['S0'] = df['S0']\n",
    "    df2['sigma'] = df['sigma']\n",
    "    df2['Network Type'] = df['Network Type']\n",
    "    #df2['p'] = df['p']\n",
    "    \n",
    "    return (df2, df2_debug)\n",
    "\n",
    "def reduce_to_scalars_old(df, conn_scale):\n",
    "    for ix in df.index:\n",
    "        for ind in df.loc[ix].index:\n",
    "            N = int(df.loc[ix,'N'])\n",
    "            if ind == 'conn':\n",
    "                df.loc[ix, ind] = df.loc[ix, ind]/conn_scale\n",
    "            elif ind == 'Number Of Samples':\n",
    "                df.loc[ix, ind] = df.loc[ix, ind][1]\n",
    "            elif ind == 'RS':\n",
    "                df.loc[ix, 'S'] = np.average(df.loc[ix, ind][:N])\n",
    "                df.loc[ix, 'R'] = np.average(df.loc[ix, ind][N:])\n",
    "                #df.loc[ix, 'R'] = df.loc[ix, ind]\n",
    "            elif ind == 'Delta' or ind == 'Delta var' or ind == 'Vega' or ind == 'Vega var':\n",
    "                df.loc[ix, ind] = np.sum(df.loc[ix, ind])/N\n",
    "            elif ind == 'Rho'or ind == 'Theta' or ind == 'Pi' or ind == 'Rho var' or ind == 'Theta var' or ind == 'Pi var':\n",
    "                df.loc[ix,\"equity \" + ind] = np.average(df.loc[ix, ind][:N])\n",
    "                df.loc[ix,\"debt \" + ind] = np.average(df.loc[ix, ind][N:])\n",
    "            elif ind == 'M' or ind == 'M var':\n",
    "                df.loc[ix, 'avg col sum'] = np.average(np.sum(df.loc[ix, ind][:,N:], axis=0))\n",
    "                df.loc[ix, 'avg row sum'] = np.average(np.sum(df.loc[ix, ind], axis=1))\n",
    "                df.loc[ix, ind] = np.sum(df.loc[ix, ind])/N\n",
    "            else:\n",
    "                df.loc[ix, ind] = np.average(df.loc[ix, ind])\n",
    "    df.drop(columns='RS', inplace=True)\n",
    "    df.drop(columns='Delta', inplace=True)\n",
    "    df.drop(columns='Delta var', inplace=True)\n",
    "    df.drop(columns='Vega', inplace=True)\n",
    "    df.drop(columns='Vega var', inplace=True)\n",
    "    df.drop(columns='Rho', inplace=True)\n",
    "    df.drop(columns='Rho var', inplace=True)\n",
    "    df.drop(columns='Theta', inplace=True)\n",
    "    df.drop(columns='Theta var', inplace=True)\n",
    "    df.drop(columns='Pi', inplace=True)\n",
    "    df.drop(columns='Pi var', inplace=True)\n",
    "    df.drop(columns='M', inplace=True)\n",
    "    df.drop(columns='M var', inplace=True)\n",
    "    \n",
    "\n",
    "def run_sim(N, row_val, col_val, p, T, r, S0, sigma, default_scale, netType):\n",
    "    import numpy as np\n",
    "    import PyVal\n",
    "    mul = max(1,col_val+S0)\n",
    "    N_MC = 20000#int(15000*(1+sigma))#int(400*mul)    # 600\n",
    "    N_nets = 1#000#int(800*mul)  # 1100\n",
    "    nw = PyVal.BS_Network()\n",
    "    print(\"Runing N=\" +str(N)+\", col sum=\"+str(col_val)+\", p=\"+str(p)+\", T=\"+str(T)+\", r=\"+str(r)+\\\n",
    "          \", S0=\"+str(S0)+\", sigma=\"+str(sigma)+\", default_scale=\"+str(default_scale)+\", netType=\"+str(netType))\n",
    "    nw.run(N, p, row_val, col_val, 2, T, r, S0, sigma, N_MC,  N_nets, default_scale, netType)\n",
    "    k_list = nw.k_vals()[0]\n",
    "    res = []\n",
    "    print(k_list)\n",
    "    for k in k_list:\n",
    "        res.append({'Network Type': netType, 'N': N, 'Number Of Samples': nw.get_N_samples(k)[0], 'default scale': default_scale,\\\n",
    "               'conn': k, 'col sum': col_val, 'T':T, 'r': r , 'sigma': sigma, 'p': p, 'S0': S0, \\\n",
    "               'M': nw.get_M(k), 'M var': nw.get_M_var(k),\\\n",
    "               'Assets': np.array(nw.get_assets(k))[0], 'Assets var': np.array(nw.get_assets_var(k))[0],\\\n",
    "               'RS': np.array(nw.get_rs(k))[0],  'RS var': np.array(nw.get_rs_var(k))[0],\\\n",
    "               'Delta': nw.get_delta_jacobians(k),  'Delta var': nw.get_delta_jacobians_var(k),\\\n",
    "               'Vega': np.array(nw.get_vega(k)),  'Vega var': np.array(nw.get_vega_var(k)),\\\n",
    "               'Theta': np.array(nw.get_theta(k)),  'Theta var': np.array(nw.get_theta_var(k)),\\\n",
    "               'Rho': np.array(nw.get_rho(k)),  'Rho var': np.array(nw.get_rho_var(k)),\\\n",
    "               'Solvent': np.array(nw.get_solvent(k))[0], 'Solvent var': np.array(nw.get_solvent_var(k))[0],\\\n",
    "               'Pi': np.array(nw.get_pi(k))[0],  'Pi var': np.array(nw.get_pi_var(k))[0]\n",
    "            #'IO Degree Distribution': np.array(nw.get_io_deg_dist())\\\n",
    "                   })\n",
    "    return res\n",
    "\n",
    "\n",
    "def combine_2_results(dict1, dict2):\n",
    "    import copy\n",
    "    res = copy.deepcopy(dict1)\n",
    "    for k in tmp_cols:\n",
    "        if k in res:\n",
    "            res.pop(k)\n",
    "    index_ok = True\n",
    "    for ind in indices:\n",
    "        if dict1[ind] != dict2[ind]:\n",
    "            index_ok = False\n",
    "    if index_ok:\n",
    "        res['Number Of Samples'] = dict1['Number Of Samples'] + dict2['Number Of Samples']\n",
    "        n1 = dict1['Number Of Samples'][1]\n",
    "        n2 = dict2['Number Of Samples'][1]\n",
    "        n_tot = n1 + n2\n",
    "        for el in col_names:\n",
    "            elv = el + ' var'\n",
    "            res[el] = (n1*dict1[el] + n2*dict2[el])/n_tot\n",
    "            res[elv] = (n1*(dict1[elv] + dict1[el]*dict1[el]) + n2*(dict2[elv] + dict2[el]*dict2[el]))/n_tot - res[el]*res[el]\n",
    "        return [res]\n",
    "    else:\n",
    "        return [dict1, dict2]\n",
    "\n",
    "    \n",
    "def results_to_df(results):\n",
    "    import copy\n",
    "    candidates = {}\n",
    "    df_list = []\n",
    "    for r_list in results:\n",
    "        for res in r_list:\n",
    "            key = tuple([res[ind] for ind in indices])\n",
    "            if key in candidates:\n",
    "                candidates[key].append(res)\n",
    "            else:\n",
    "                candidates[key] = [res]\n",
    "    for key, value in candidates.items():\n",
    "        res_in = copy.deepcopy(value)\n",
    "        while len(res_in) > 1:\n",
    "            el1 = res_in.pop()\n",
    "            el2 = res_in.pop()\n",
    "            res_in = res_in + combine_2_results(el1, el2)\n",
    "        candidates[key] = res_in[0]\n",
    "        df_list.append(res_in[0])\n",
    "    #res.set_index(indices, inplace=True)\n",
    "    return pd.DataFrame(df_list)\n",
    "        \n",
    "            \n",
    "    \n",
    "def flatten_input(cell):\n",
    "    if type(cell) is list:\n",
    "        return cell[0]\n",
    "    else:\n",
    "        return cell\n",
    "\n",
    "def pList(N, pts):\n",
    "    import numpy as np\n",
    "    #    return np.union1d(np.union1d(np.linspace(0.0, 10.0/N, pts), np.linspace(0.6/N,1.4/N,pts)), np.linspace((np.log(N)-np.log(N)/7.)/N,(np.log(N)+np.log(N)/7.)/N,pts))\n",
    "    res = np.union1d(np.linspace(0.0, 2.0/N, 2*pts), np.union1d(np.linspace(0.0, 8.0/N, pts), np.linspace((np.log(N)-np.log(N)/7.)/N, (np.log(N)+np.log(N)/7.)/N, pts)))\n",
    "    res = res[res <= 5.0/N]\n",
    "    return res\n",
    "\n",
    "def pFixedList(N, pts):\n",
    "    import numpy as np\n",
    "    return np.linspace(0.0, (pts-1)/N, pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "## Black Scholes\n",
    "    - T = 1\n",
    "    - r = 0\n",
    "    \n",
    "## Network Parameters\n",
    "    - N = [15, 25, 50, 75, 100, 150, 200, 300]\n",
    "    - <k> = n*p = [0 .. 1.8]\n",
    "    - M => ER(N,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10800"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dview.block=False\n",
    "Nlist = [100]\n",
    "NPoints = 12\n",
    "\n",
    "batch = 0\n",
    "n_batches = 1\n",
    "\n",
    "netTypes = [3,5]#[2,3,5] #2 - star, 3 - ring, 5 - uniform\n",
    "default_scales_full = [1.0]#np.linspace(0.7,1.1,3)\n",
    "default_scales = default_scales_full#[:5]\n",
    "row_vals_all = np.linspace(0.1,0.8, 25) #15  #[0.1, 0.2, 0.4, 0.7]\n",
    "row_vals = row_vals_all[batch::n_batches]\n",
    "S0_vals = np.union1d(np.union1d(np.linspace(0., 0.45, 5), np.linspace(0.4,1.1,14)),np.linspace(1.2,2.5,5)) #15\n",
    "r_vals = np.linspace(0.0,0.4,3)#[0.0, 0.05, 0.1]\n",
    "T_vals = np.linspace(0.1,1.0,3)#\n",
    "sigma_vals = [0.2]#np.linspace(0.25,0.35,2)#np.linspace(0.1,0.5,3)\n",
    "param_grid_full = [(N, row_sum, row_sum, el, T, r, S0, sigma ,default_scale, netT) for N in Nlist[::-1] for el in [0.1] for row_sum in row_vals[::-1] \n",
    "              for default_scale in default_scales for r in r_vals for T in T_vals for S0 in S0_vals for sigma in sigma_vals for netT in netTypes]\n",
    "len(param_grid_full)\n",
    "param_grid = param_grid_full\n",
    "\n",
    "len(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"Params_IMP_med_bak.npy\", param_grid_full)\n",
    "filename = \"param_grid_unif_ring\" + str(batch+1) + \"_of_\" + str(n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_res = []\n",
    "ids = []\n",
    "for el in param_grid:\n",
    "    with dview.temp_flags(retries=2):\n",
    "        b_re = dview.apply_async(run_sim, *el)\n",
    "        async_res.append(b_re)\n",
    "        ids.extend(b_re.msg_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2551/10800 complete\n"
     ]
    }
   ],
   "source": [
    "done = 0\n",
    "waiting = 0\n",
    "for el in async_res:\n",
    "    if el.progress == 1:\n",
    "        done += 1\n",
    "    else:\n",
    "        waiting += 1\n",
    "print(str(done) + \"/\"+str(waiting+done) + \" complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unfinished jobs: 8249/10800. Running on 256 cores => 9.97 of 42.19 cycles\n"
     ]
    }
   ],
   "source": [
    "print('unfinished jobs: '+ str(len(rc.result_status(ids)['pending'])) + '/'\\\n",
    "      + str(len(rc.result_status(ids)['completed']) \\\n",
    "      + len(rc.result_status(ids)['pending'])) \\\n",
    "      + str('. Running on ') + str(len(rc.ids)) + ' cores'\\\n",
    "      +\" => {:1.2f} of {:1.2f} cycles\".format(len(rc.result_status(ids)['completed'])/len(rc.ids), \\\n",
    "          (len(rc.result_status(ids)['completed'])+len(rc.result_status(ids)['pending']))/len(rc.ids))  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_content = [r.get() for r in async_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results_to_df(f_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.set_index(['Network Type'], inplace=True)\n",
    "df2,df2_debug = reduce_to_scalars(df, Nlist[0], 5.) #reduce_star_to_scalars\n",
    "df2.to_csv(filename+'.csv', index=False)\n",
    "df.to_csv(filename+'_full.csv', index=False)\n",
    "#df2_debug.to_json(filename+'.json')\n",
    "#df.to_pickle('BAK_'+filename+'.pkl.bz2', compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(netTypes[0] == 2):\n",
    "    df2,df2_debug = reduce_star_to_scalars(df, Nlist[0], 5.)\n",
    "else:\n",
    "    df2,df2_debug = reduce_to_scalars(df, Nlist[0], 5.) #reduce_star_to_scalars\n",
    "df2.to_csv(filename+'.csv', index=False)\n",
    "df.to_csv(filename+'_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_star=\"param_grid_star\"\n",
    "df2_star = df2\n",
    "df_star = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('BAK_'+filename+'.pkl.bz2', compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'param_grid_ER_10x_03_sigma.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-54adb7606932>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#inf = open('param_grid_ER_5x_ds_vs_w.pkl', 'rb') #'res_0_1_newnew.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_scales\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'default scale'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcol_sums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'col sum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mspot_prices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'S0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ipcluster_1/ipcluster_1/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ipcluster_1/ipcluster_1/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ipcluster_1/ipcluster_1/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ipcluster_1/ipcluster_1/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ipcluster_1/ipcluster_1/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'param_grid_ER_10x_03_sigma.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#inf = open('param_grid_ER_5x_ds_vs_w.pkl', 'rb') #'res_0_1_newnew.pkl'\n",
    "df1 = pd.read_csv(filename+'.csv')\n",
    "df_scales = np.unique(df1['default scale'])\n",
    "col_sums = np.unique(df1['col sum'])  \n",
    "spot_prices = np.unique(df1['S0'])    \n",
    "df1.set_index(['N', 'conn', 'col sum', 'default scale', 'r', 'S0'], inplace=True)\n",
    "spot_prices\n",
    "#df1.reset_index([0,2,3]).index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstobbe/ipcluster_1/ipcluster_1/lib/python3.6/site-packages/pandas/core/indexing.py:979: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_nested_tuple(tup)\n"
     ]
    }
   ],
   "source": [
    "N = 70\n",
    "val = 0.1\n",
    "ds = 1.0\n",
    "r = 0.0\n",
    "S0 = 0.25\n",
    "data = df3.loc[idx[N,:,val, ds, r, S0],['equity Delta','equity Delta var','debt Delta','debt Delta var', 'Number Of Samples']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'col_sums' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-7d4acaca0437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_sums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspot_prices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspot_prices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_sums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_sums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspot_prices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'col_sums' is not defined"
     ]
    }
   ],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "cmap = sns.color_palette(\"muted\")#sns.cubehelix_palette(8, light=1)\n",
    "ccycle = itertools.cycle(cmap)\n",
    "#sns.set_palette(cmap)\n",
    "\n",
    "def plot_val(l, ax, val, ds, S0 , title=\"\"):\n",
    "    nl = np.unique(l.reset_index(level=[1,2,3,4,5]).index.values)\n",
    "    rl = np.unique(df1.reset_index()['r'])\n",
    "    for r in rl:\n",
    "        for N in nl:\n",
    "            c = next(ccycle)\n",
    "            data = l.loc[idx[N,:,val, ds, r, S0],['equity Delta','equity Delta var','debt Delta',\\\n",
    "                                                  'debt Delta var', 'Number Of Samples']]\n",
    "            #data = l.loc[idx[N,:,val, ds, r],['Vega','Vega var', 'Number Of Samples']]\n",
    "            #data = l.loc[idx[N,:,val, ds, r],['Solvent', 'Number Of Samples']]\n",
    "            #data = l.loc[idx[N,:,val, ds, r],['RS', 'Number Of Samples']]\n",
    "            n_samples = np.sqrt(np.array([ el for el in data['Number Of Samples'].values]))\n",
    "            x = l.loc[idx[N,:,val, ds, r, S0],:].reset_index([0,2,3,4,5]).index.values\n",
    "            y = np.array([np.sum(el)/(float(N)) for el in data['equity Delta'].values])\n",
    "            #y_err = np.array([np.sqrt(np.sum(el)/float(N)) for el in data['Delta var'].values])/n_samples\n",
    "            #y = np.array([np.sum(el[:N]+el[N:])/(float(N)) for el in data['RS'].values])\n",
    "            #y = np.array([np.sum(el)/(float(N)) for el in data['Solvent'].values])\n",
    "            #y = np.array([np.sum(el)/(float(N)) for el in data['Vega'].values])\n",
    "            #y_err = np.array([np.sqrt(np.sum(el)/float(N)) for el in data['Vega var'].values])/n_samples\n",
    "            ii = np.argsort(x)\n",
    "            x = x[ii]\n",
    "            y = y[ii]\n",
    "            #y_err = y_err[ii]\n",
    "            #sns.lineplot(x,y, ax = ax)\n",
    "            ax.plot(x,y, marker='o', markersize=4.4, label=\"N=\"+str(N)+\", r=\"+str(r),color=c)\n",
    "            #ax.fill_between(x, y-y_err, y+y_err, color=c, alpha=0.4)\n",
    "            if title:\n",
    "                ax.set_title(title)\n",
    "            else:\n",
    "                ax.set_title(r'$\\sum_j M_{ij} = $' + str(val) +\" default scale \" +str(ds))\n",
    "            ax.legend()\n",
    "\n",
    "       \n",
    "fig, axs = plt.subplots(nrows=len(col_sums), ncols=len(spot_prices), sharex=True, sharey=False, figsize=(7*len(spot_prices), 10*len(col_sums)))\n",
    "for ic, val in enumerate(col_sums):\n",
    "    for ir, sp in enumerate(spot_prices):\n",
    "        ax = axs[ic,ir]\n",
    "        plot_val(df1 ,ax, val, 1.0, sp, \"w=\" + str(val) + \", S_0=\" + str(sp))\n",
    "\n",
    "#axs[0,0].set_ylabel(r'$\\Sigma_{ij} \\Delta_{ij}$')\n",
    "#axs[1,0].set_ylabel(r'$\\Sigma_{ij} \\Delta_{ij}$')\n",
    "#axs[2,0].set_ylabel(r'$\\Sigma_{ij} \\Delta_{ij}$')\n",
    "#axs[2,0].set_xlabel(r'$\\langle k \\rangle = n \\cdot p$')\n",
    "#axs[2,1].set_xlabel(r'$\\langle k \\rangle = n \\cdot p$')\n",
    "#plt.errorbar(x, y, yerr=y_err, fmt='o')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spot_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.3, 0.5, 0.7])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df1.reset_index([0,1,2,3,4,5])['col sum'])   #r, S0, default scale, col sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res = [r.get() for r in async_res]\n",
    "results = []\n",
    "res = res_0_4\n",
    "tmp = combine_results(*res)\n",
    "results.append(tmp[0])\n",
    "while len(tmp[1]):\n",
    "    tmp = combine_results(*tmp[1])\n",
    "    results.append(tmp[0])\n",
    "results[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
